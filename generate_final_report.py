import zipfile
import os

def create_market_risk_report():
    downloads_path = os.path.join(os.path.expanduser("~"), "Downloads")
    zip_filename = os.path.join(downloads_path, "Market_Risk_Report_Final.zip")

    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:

        main_tex = r"""\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{float}
\usepackage{fancyvrb}

\geometry{margin=2.5cm}

\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    tabsize=4
}

\DefineVerbatimEnvironment{console}{Verbatim}{
    frame=single,
    fontsize=\footnotesize
}

\title{
    \includegraphics[width=0.25\textwidth]{esilv_logo.png}\\[1cm]
    \textbf{Market Risk Measurement:\\
    Application to Natixis Stock}\\
    \vspace{0.5cm}
    \large Quantitative Finance Project
}

\author{
    Lucas Soares \and Maxime Gruez\\
    ESILV - Financial Engineering\\
    Academic Year 2025-2026
}

\date{\today}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Introduction}

This project focuses on measuring market risk for Natixis stock using different statistical methods. We implemented everything from scratch in pure Python without external packages like numpy or pandas. The dataset covers daily Natixis prices from 2015 to 2018 (1023 observations total), plus high-frequency forex data for the final question.

We used the 2015-2016 period to build our models and tested them on 2017-2018 data. The main objective was to calculate Value-at-Risk through multiple approaches and compare their results. Each method reveals different aspects of the risk profile, from simple non-parametric estimation to advanced extreme value theory.

\section{Question A: Non-Parametric VaR}

\subsection{Theory}

Value-at-Risk at confidence level $\alpha$ is the quantile:
\begin{equation}
\text{VaR}_\alpha = \inf\{x : F(x) \geq \alpha\}
\end{equation}

Instead of assuming normality, we estimate the density function using the Parzen-Rosenblatt kernel estimator with biweight kernel:
\begin{equation}
K(u) = \frac{15}{16}(1-u^2)^2 \mathbb{1}_{|u| \leq 1}
\end{equation}

The density estimate is:
\begin{equation}
\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{x - X_i}{h}\right)
\end{equation}

where $h$ is the bandwidth selected using Silverman's rule:
\begin{equation}
h = 1.06 \times \hat{\sigma} \times n^{-1/5}
\end{equation}

\subsection{Implementation}

\begin{lstlisting}
def biweight_kernel(u):
    if abs(u) <= 1:
        return 0.9375 * ((1 - u*u) ** 2)
    return 0.0

def bandwidth(data):
    n = len(data)
    m = sum(data) / n
    var = sum((x - m)**2 for x in data) / (n - 1)
    sd = math.sqrt(var)
    h = 1.06 * sd * (n ** (-1.0/5.0))
    return h

def kernel_density(x, data, h):
    n = len(data)
    total = 0.0
    for xi in data:
        u = (x - xi) / h
        total += biweight_kernel(u)
    return total / (n * h)

def var_kernel(returns, alpha=0.05):
    h = bandwidth(returns)
    lower = min(returns)
    upper = max(returns)

    steps = 1000
    dx = (upper - lower) / steps
    x = lower
    cumul = 0.0
    while cumul < alpha and x < upper:
        cumul += kernel_density(x, returns, h) * dx
        x += dx

    return x
\end{lstlisting}

\subsection{Results}

Training period (2015-2016):

\begin{console}
Question A Non parametric VaR

Loaded data: 1023 prices

a) VaR estimation (2015-2016)
Number of returns: 512
Alpha = 0.05
VaR = -0.0386 soit -3.86%
\end{console}

Backtesting (2017-2018):

\begin{console}
b) Backtesting (2017-2018)
Test returns: 509
Violations: 8
Actual rate: 1.57%
Expected rate: 5.0%
Underestimates risk
\end{console}

\subsection{Analysis}

The VaR at 95\% confidence is -3.86\%, meaning we expect daily losses to exceed this threshold about 5\% of the time. However, backtesting shows only 8 violations out of 509 days (1.57\%). This means the model was too conservative.

The low violation rate has several explanations. First, market volatility was lower in 2017-2018 compared to the training period. Second, kernel smoothing can over-smooth the tails, making them appear heavier than they really are. Third, with only 512 observations, the extreme quantiles are hard to pin down precisely.

While the model underestimates risk in this case, being conservative isn't necessarily bad for risk management. The gap between expected and actual violations (3.43\%) is noticeable but not extreme. We can consider this VaR estimate reasonable despite its limitations.

\section{Question B: Expected Shortfall}

\subsection{Theory}

Expected Shortfall measures the average loss beyond VaR:
\begin{equation}
\text{ES}_\alpha = \mathbb{E}[X \mid X \leq \text{VaR}_\alpha] = \frac{1}{\alpha}\int_0^\alpha \text{VaR}_u \, du
\end{equation}

Unlike VaR, ES is a coherent risk measure satisfying sub-additivity. This makes it theoretically superior for portfolio optimization and capital allocation.

\subsection{Implementation}

\begin{lstlisting}
def expected_shortfall(returns, alpha=0.05):
    var_val = var_kernel(returns, alpha)
    tail_losses = []
    for r in returns:
        if r < var_val:
            tail_losses.append(r)

    if len(tail_losses) > 0:
        es = sum(tail_losses) / len(tail_losses)
    else:
        es = var_val
    return es
\end{lstlisting}

\subsection{Results}

\begin{console}
Question B Expected Shortfall

Period 2015-2016: 512 returns
Alpha = 0.05

VaR = -0.0386 soit -3.86%
ES  = -0.0552 soit -5.52%

Difference: 0.0166
Ratio ES/VaR = 1.431
ES higher than VaR
\end{console}

\subsection{Analysis}

The Expected Shortfall is -5.52\% versus -3.86\% for VaR, a difference of 1.66 percentage points. The ES/VaR ratio of 1.43 exceeds the normal distribution benchmark of about 1.25. This indicates fat tails and negative skewness in the return distribution.

For a 1 million euro position, the additional tail risk beyond VaR amounts to 16,600 euros on average when losses exceed the threshold. This is substantial and shows why VaR alone can be misleading. The elevated ratio confirms that Natixis exhibits typical bank stock behavior - occasional large losses during crises but limited upside due to regulatory constraints.

This result supports Basel III's move toward ES for regulatory capital. ES captures not just the frequency of breaches but also their severity, which is what actually matters for capital adequacy and survival during stress periods.

\section{Question C: Extreme Value Theory}

\subsection{Theory}

The Generalized Extreme Value distribution models block maxima:
\begin{equation}
G_{\xi,\mu,\sigma}(x) = \exp\left\{-\left[1 + \xi\left(\frac{x-\mu}{\sigma}\right)\right]^{-1/\xi}\right\}
\end{equation}

Parameters:
\begin{itemize}
\item $\xi$: shape parameter (tail index)
\item $\mu$: location parameter
\item $\sigma > 0$: scale parameter
\end{itemize}

The Pickands estimator for $\xi$ is:
\begin{equation}
\hat{\xi} = \frac{1}{\log 2} \log\left(\frac{X_{(n-k)} - X_{(n-2k)}}{X_{(n-2k)} - X_{(n-4k)}}\right)
\end{equation}

Classification:
\begin{equation}
\xi \begin{cases}
> 0 & \text{Fr√©chet (fat tails)} \\
= 0 & \text{Gumbel (exponential tails)} \\
< 0 & \text{Weibull (bounded)}
\end{cases}
\end{equation}

\subsection{Implementation}

\begin{lstlisting}
def pickands(extremes):
    n = len(extremes)
    extremes_sorted = sorted(extremes)
    k = n // 4

    x1 = extremes_sorted[n - k - 1]
    x2 = extremes_sorted[n - 2*k - 1]
    x3 = extremes_sorted[n - 4*k - 1]

    num = x1 - x2
    denom = x2 - x3

    if denom != 0 and num > 0:
        ratio = num / denom
        if ratio > 0:
            xi = math.log(ratio) / math.log(2)
        else:
            xi = 0.0
    else:
        xi = 0.0

    return xi

def get_gev_params(extremes):
    xi = pickands(extremes)

    n = len(extremes)
    mean_ext = sum(extremes) / n
    var = sum((x - mean_ext)**2 for x in extremes) / (n - 1)
    std_ext = math.sqrt(var)

    sigma = std_ext * 0.8
    mu = mean_ext - 0.58 * sigma

    return xi, mu, sigma

def var_evt(xi, mu, sigma, alpha):
    if abs(xi) > 0.01:
        log_term = -math.log(1 - alpha)
        var_val = mu - (sigma / xi) * (1 - log_term**(-xi))
    else:
        var_val = mu - sigma * math.log(-math.log(1 - alpha))
    return var_val
\end{lstlisting}

\subsection{Results}

\begin{console}
Question C Extreme Value Theory

Total returns: 1022

a) GEV parameters with Pickands

Right tail (gains):
Blocks: 51
xi=-0.702, mu=0.0294, sigma=0.0127
Bounded distribution

Left tail (losses):
Blocks: 51
xi=-0.293, mu=-0.0257, sigma=0.0211
Bounded distribution


b) VaR with EVT

VaR losses:
  90.0%: -0.045689 (-4.5689%)
  95.0%: -0.053064 (-5.3064%)
  99.0%: -0.066407 (-6.6407%)
  99.5%: -0.071134 (-7.1134%)
\end{console}

\subsection{Analysis}

Both tails have negative $\xi$ values, placing Natixis in the Weibull domain. This means extreme returns are bounded - there's a theoretical limit to single-day moves in both directions. The right tail (gains) has $\xi = -0.70$ while the left tail (losses) has $\xi = -0.29$.

The magnitude difference (0.70 vs 0.29) shows asymmetry. The right tail is 2.4 times more constrained than the left tail, meaning extreme gains are much more limited than extreme losses. This matches the negative skewness we saw in the ES analysis.

The EVT VaR at 95\% confidence is -5.31\%, compared to -3.86\% from kernel density. EVT is 36\% more conservative because it focuses purely on extremes while kernel uses the full distribution. Notably, the EVT VaR is very close to the Expected Shortfall (-5.52\%), with only 4\% difference. This cross-validation between independent methods gives us confidence the true tail risk is around 5.3\%.

The VaR progression with confidence level is also informative. Moving from 90\% to 95\% adds 0.7 percentage points, while 95\% to 99\% adds 1.3 points. The tail gets progressively fatter at more extreme quantiles, typical of financial returns.

\section{Question D: Bouchaud Price Impact Model}

\subsection{Theory}

Bouchaud's model describes the price impact of transaction volumes using a power-law relationship. We start with the basic impact model:

\begin{equation}
I \approx \lambda \cdot V^\delta
\end{equation}

where $I$ is the price impact, $V$ is the transaction volume, $\lambda$ is the impact coefficient, and $\delta$ is the power-law exponent.

To estimate the parameters $\lambda$ and $\delta$, we apply a logarithmic transformation:

\begin{equation}
\ln(I) = \ln(\lambda) + \delta \cdot \ln(V)
\end{equation}

This gives us a linear relationship between $\ln(V)$ and $\ln(I)$. We can then calculate $\delta$ as the slope using covariance and variance:

\begin{equation}
\delta = \frac{\text{Cov}(\ln(V), \ln(I))}{\text{Var}(\ln(V))}
\end{equation}

Once we have $\delta$, we calculate $\lambda$ by rearranging the log equation:

\begin{equation}
\ln(\lambda) = \ln(I) - \delta \cdot \ln(V)
\end{equation}

Taking the exponential:

\begin{equation}
\lambda = e^{\overline{\ln(I)} - \delta \cdot \overline{\ln(V)}}
\end{equation}

where the bars indicate mean values.

For the relaxation time $\tau$, we use the autocorrelation of returns. The autocorrelation at lag 1 is:

\begin{equation}
\rho_1 = \frac{\sum_{i=1}^{n-1} (r_i - \bar{r})(r_{i+1} - \bar{r})}{(n-1) \cdot \text{Var}(r)}
\end{equation}

The relaxation time is then estimated as:

\begin{equation}
\tau = -\frac{\Delta t}{\ln(\rho_1)}
\end{equation}

where $\Delta t$ is the average time between transactions. This gives us the characteristic time for price impacts to decay.

Kyle's square-root law predicts $\delta \approx 0.5$, which we can verify with our data.

\subsection{Implementation}

\begin{lstlisting}
def get_bouchaud_params(transactions):
    log_vols = []
    log_impacts = []

    for i in range(1, len(transactions)):
        p0 = transactions[i-1]['price']
        p1 = transactions[i]['price']
        impact = abs(p1 - p0)
        vol = transactions[i]['volume']

        if vol is not None and vol > 0 and impact > 0:
            log_vols.append(math.log(vol))
            log_impacts.append(math.log(impact))

    n = len(log_vols)
    if n == 0:
        return 0.01, 0.5

    mean_v = sum(log_vols) / n
    mean_i = sum(log_impacts) / n

    cov = sum((log_vols[i] - mean_v) * (log_impacts[i] - mean_i)
              for i in range(n)) / n
    var_v = sum((lv - mean_v)**2 for lv in log_vols) / n

    if var_v > 0:
        delta = cov / var_v
        lam = math.exp(mean_i - delta * mean_v)
    else:
        delta = 0.5
        lam = 0.01

    return lam, delta
\end{lstlisting}

\subsection{Results}

\begin{console}
Question D Bouchaud Model

Transactions: 1001

Results:
lambda = 0.0430
delta = 0.032 (weak volume effect)
tau = 0.0010 days
sigma = 0.0007 soit 0.07%

Autocorrelation: -0.041

Model needs adjustments
\end{console}

\subsection{Analysis}

The estimated $\delta = 0.032$ is far below the theoretical prediction of 0.5 from Kyle's model. This doesn't mean the implementation is wrong - it reflects a data quality problem. Looking at the transaction file, most volume entries are missing or zero. The regression operates on a sparse subset of observations where volume happens to be reported.

The other parameters make more sense. Lambda of 0.043 suggests moderate market liquidity. Tau of 0.001 days (roughly 1.4 minutes) indicates very fast mean-reversion, consistent with modern algorithmic trading. The intraday volatility of 7.4 basis points is reasonable. Autocorrelation near zero (-0.041) confirms efficient price discovery.

The "needs adjustments" diagnostic is appropriate since $\delta$ falls outside the empirically reasonable range of 0.3-0.7. This is a data limitation rather than a model failure. The Bouchaud framework requires complete, high-quality volume data to estimate the power-law exponent accurately. When that data is missing, the parameters become unreliable.

This demonstrates an important principle in quantitative finance - even the best models can't overcome poor data quality. The right response is to acknowledge the limitation honestly rather than trust questionable parameter estimates.

\section{Question E: Haar Wavelets and Hurst Exponent}

\subsection{Theory}

The Haar wavelet decomposes signals into multi-scale components:
\begin{equation}
\phi(t) = \mathbb{1}_{[0,1)}, \quad
\psi(t) = \mathbb{1}_{[0,0.5)} - \mathbb{1}_{[0.5,1)}
\end{equation}

Decomposition:
\begin{equation}
a_j = \frac{a_{j+1,2k} + a_{j+1,2k+1}}{2}, \quad
d_j = \frac{a_{j+1,2k} - a_{j+1,2k+1}}{2}
\end{equation}

The Hurst exponent is estimated via rescaled range:
\begin{equation}
H = \frac{\log(R/S)}{\log(n)}
\end{equation}

where $R$ is the range of cumulative deviations and $S$ is the standard deviation.

Interpretation:
\begin{equation}
H \begin{cases}
> 0.5 & \text{persistent (trending)} \\
= 0.5 & \text{random walk} \\
< 0.5 & \text{mean-reverting}
\end{cases}
\end{equation}

\subsection{Implementation}

\begin{lstlisting}
def read_forex_data(filename):
    gbp_prices = []
    sek_prices = []
    cad_prices = []

    f = open(filename, 'r')
    lines = f.readlines()[2:]
    for line in lines:
        parts = line.strip().split(';')
        if len(parts) >= 11:
            gbp_high = float(parts[1].replace(',', '.'))
            gbp_low = float(parts[2].replace(',', '.'))
            gbp_prices.append((gbp_high + gbp_low) / 2.0)

            sek_high = float(parts[5].replace(',', '.'))
            sek_low = float(parts[6].replace(',', '.'))
            sek_prices.append((sek_high + sek_low) / 2.0)

            cad_high = float(parts[9].replace(',', '.'))
            cad_low = float(parts[10].replace(',', '.'))
            cad_prices.append((cad_high + cad_low) / 2.0)
    f.close()

    return {'GBP': gbp_prices, 'SEK': sek_prices, 'CAD': cad_prices}

def correlation(x, y):
    n = len(x)
    mx = sum(x) / n
    my = sum(y) / n

    cov = 0.0
    vx = 0.0
    vy = 0.0

    for i in range(n):
        dx = x[i] - mx
        dy = y[i] - my
        cov += dx * dy
        vx += dx * dx
        vy += dy * dy

    if vx == 0 or vy == 0:
        return 0.0

    return cov / math.sqrt(vx * vy)

def corr_at_scale(r1, r2, scale):
    if scale == 0:
        return correlation(r1, r2)

    step = 2 ** scale
    agg1 = []
    agg2 = []

    for i in range(0, min(len(r1), len(r2)) - step + 1, step):
        total1 = 0.0
        total2 = 0.0
        for j in range(step):
            total1 += r1[i + j]
            total2 += r2[i + j]
        agg1.append(total1)
        agg2.append(total2)

    if len(agg1) == 0:
        return 0.0

    return correlation(agg1, agg2)

def hurst_exponent(returns):
    n = len(returns)
    mean_r = sum(returns) / n

    cumul = []
    s = 0.0
    for r in returns:
        s += (r - mean_r)
        cumul.append(s)

    R = max(cumul) - min(cumul)

    var = 0.0
    for r in returns:
        var += (r - mean_r) ** 2
    var = var / n
    S = math.sqrt(var)

    if S == 0:
        return 0.5

    H = math.log(R / S) / math.log(n)
    return H

def annualized_vol(returns, periods=252):
    n = len(returns)
    m = sum(returns) / n

    var = 0.0
    for r in returns:
        var += (r - m) ** 2
    var = var / (n - 1)

    daily_vol = math.sqrt(var)
    annual_vol = daily_vol * math.sqrt(periods)
    return annual_vol
\end{lstlisting}

\subsection{Results}

\begin{console}
Question E Haar Wavelets & Hurst

Data: 12929 points

a) Multi-scale correlations

GBP/SEK:
  scale 0: 0.192
  scale 1: 0.210
  scale 2: 0.267
  scale 3: 0.232

GBP/CAD:
  scale 0: 0.245
  scale 1: 0.261
  scale 2: 0.206
  scale 3: 0.188

SEK/CAD:
  scale 0: 0.193
  scale 1: 0.204
  scale 2: 0.163
  scale 3: 0.168

Epps effect observed: correlation increases with scale


b) Hurst exponent

GBP: H=0.5530 (trending)
SEK: H=0.5111 (trending)
CAD: H=0.4950 (mean reversion)


Annualized volatility:
GBP: 0.097 soit 9.7%
SEK: 0.051 soit 5.1%
CAD: 0.079 soit 7.9%
\end{console}

\subsection{Analysis}

For GBP/SEK, correlation starts at 0.19 at the highest frequency, increases to 0.21, peaks at 0.27 at scale 2, then drops to 0.23. This is the Epps effect - correlations appear artificially low at high frequencies due to microstructure noise like bid-ask bounce and asynchronous trading. As we aggregate to longer time scales, the noise averages out and the true economic correlation emerges.

The peak at scale 2 represents the optimal aggregation level where we've eliminated most noise without losing too many observations. The decline at scale 3 likely comes from reduced sample size after heavy aggregation. The other currency pairs show moderate correlations (0.16-0.26) without a clear pattern, which makes sense since they're all measured against EUR but represent distinct economies.

The Hurst exponents cluster around 0.5: GBP at 0.553, SEK at 0.511, and CAD at 0.495. Values near 0.5 indicate random walk behavior, supporting the efficient market hypothesis. GBP shows slight persistence possibly due to Brexit-related news cycles. CAD's marginal mean-reversion might relate to its correlation with oil prices. But these deviations are small and could just be sampling variation.

The volatilities rank as GBP (9.7\%) > CAD (7.9\%) > SEK (5.1\%), matching economic fundamentals. GBP was volatile during Brexit, SEK represents a stable Nordic economy, and CAD sits in between as a commodity currency.

\section{Conclusion}

This project explored five complementary approaches to measuring market risk. Each method revealed different aspects of Natixis's risk profile.

The kernel VaR provided a flexible non-parametric estimate of -3.86\% but proved conservative in backtesting. Expected Shortfall at -5.52\% showed significant tail risk beyond VaR, with a ratio of 1.43 indicating fat tails. EVT analysis gave VaR estimates around -5.3\%, confirming the ES result through an independent method. The GEV parameters revealed negative skewness and bounded tails in the Weibull domain.

The Bouchaud model highlighted data quality issues - without complete volume information, we couldn't reliably estimate the price impact exponent. The forex analysis demonstrated the Epps effect in correlation and confirmed near-efficient market dynamics with Hurst exponents close to 0.5.

Working without external libraries forced us to understand each algorithm deeply. Implementing kernel density estimation, Pickands estimator, and wavelet transforms from scratch provided insights that using black-box functions would have missed.

For Natixis specifically, the true tail risk appears to be around 5.3\% at 95\% confidence, substantially higher than the kernel VaR suggests. The asymmetric tail behavior (gains more bounded than losses) is typical for bank stocks facing regulatory constraints on upside but sharp crisis-driven downside.

The main limitation was the historical period (2015-2018) which misses extreme events like COVID-19. Future work could extend the analysis to more recent data and explore conditional volatility models to capture time-varying risk.

\section*{References}

\begin{enumerate}
\item Wikipedia contributors. (2025). \textit{Kernel density estimation}. Wikipedia, The Free Encyclopedia. \url{https://en.wikipedia.org/wiki/Kernel_density_estimation}
\end{enumerate}

\end{document}
"""

        zipf.writestr('main.tex', main_tex)

        readme = """# Market Risk Project - ESILV 2025-2026

## Authors
Lucas Soares & Maxime Gruez

## Contents
- main.tex: Complete project report
- esilv_logo.png: School logo (to add)

## Compilation
1. Extract ZIP
2. Add esilv_logo.png
3. Upload to Overleaf or compile with pdflatex

## Notes
Pure Python implementation without packages
"""
        zipf.writestr('README.md', readme)

    file_size_kb = os.path.getsize(zip_filename) / 1024
    print(f"\n{'='*60}")
    print(f"[OK] Final report generated!")
    print(f"[OK] Location: {zip_filename}")
    print(f"[OK] Size: {file_size_kb:.2f} KB")
    print(f"{'='*60}\n")

if __name__ == "__main__":
    create_market_risk_report()
